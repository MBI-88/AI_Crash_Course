{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pygame as pg\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente\n",
    "class Enviroment():\n",
    "    def __init__(self,waitTime):\n",
    "        self.width = 880\n",
    "        self.height =  880\n",
    "        self.nRows = 10\n",
    "        self.nColumns = 10\n",
    "        self.initSnakelen = 2\n",
    "        self.defReward = -0.03\n",
    "        self.negReward = -1\n",
    "        self.posReward = 2\n",
    "        self.waitTime = waitTime\n",
    "\n",
    "        if self.initSnakelen > self.nRows/2:\n",
    "            self.initSnakelen = int(self.nRows/2)\n",
    "        \n",
    "        self.screen = pg.display.set_mode((self.width,self.height))\n",
    "        self.snakePos = list()\n",
    "        self.screenMap = np.zeros((self.nRows,self.nColumns))\n",
    "\n",
    "        for i in range(self.initSnakelen):\n",
    "            self.snakePos.append((int(self.nRows/2)+i,int(self.nColumns/2)))\n",
    "            self.screenMap[int(self.nRows/2)+i][int(self.nColumns/2)] = 0.5\n",
    "        \n",
    "        self.applePos = self.placeAple()\n",
    "        self.drawScreen()\n",
    "        self.collected = False\n",
    "        self.lastMove = 0\n",
    "    \n",
    "    def placeAple(self):\n",
    "        posx = np.random.randint(0,self.nColumns)\n",
    "        posy = np.random.randint(0,self.nRows)\n",
    "        while self.screenMap[posx][posy] == 0.5:\n",
    "            posx = np.random.randint(0,self.nColumns)\n",
    "            posy = np.random.randint(0,self.nRows)\n",
    "        self.screenMap[posx][posy] = 1\n",
    "        return (posx,posy)\n",
    "    \n",
    "    def drawScreen(self):\n",
    "        self.screen.fill((0,0,0))\n",
    "        cellWidth = self.width/self.nColumns\n",
    "        cellHeight = self.height/self.nRows\n",
    "\n",
    "        for i in range(self.nRows):\n",
    "            for j in range(self.nColumns):\n",
    "                if self.screenMap[i][j] ==  0.5:\n",
    "                    pg.draw.rect(self.screen,(255,255,255),(j*cellWidth + 1,i*cellHeight + 1,cellWidth - 2,cellHeight - 2))\n",
    "                elif self.screenMap[i][j] == 1:\n",
    "                    pg.draw.rect(self.screen,(255,0,0),(j*cellWidth + 1,i*cellHeight +  1,cellWidth - 2,cellHeight - 2))\n",
    "        \n",
    "        pg.display.flip()\n",
    "    \n",
    "    def moveSnake(self,nextPos,col):\n",
    "        self.snakePos.insert(0,nextPos)\n",
    "\n",
    "        if not col :\n",
    "            self.snakePos.pop(len(self.snakePos)-1)\n",
    "\n",
    "        self.screenMap = np.zeros((self.nRows,self.nColumns))\n",
    "\n",
    "        for i in range(len(self.snakePos)):\n",
    "            self.screenMap[self.snakePos[i][0]][self.snakePos[i][1]] = 0.5\n",
    "        \n",
    "        if col:\n",
    "            self.applePos = self.placeAple()\n",
    "            self.collected = True\n",
    "        \n",
    "        self.screenMap[self.applePos[0]][self.applePos[1]] = 1\n",
    "    \n",
    "    def step(self,action):\n",
    "        gameOver = False\n",
    "        reward = self.defReward\n",
    "        self.collected = False\n",
    "\n",
    "        for event in pg.event.get():\n",
    "            if event.type == pg.QUIT:\n",
    "                return\n",
    "        snakeX = self.snakePos[0][1]\n",
    "        snakeY = self.snakePos[0][0]\n",
    "\n",
    "        if action == 1 and self.lastMove == 0:\n",
    "            action = 0\n",
    "        if action == 0 and self.lastMove == 1:\n",
    "            action = 1\n",
    "        if action == 3 and self.lastMove == 2:\n",
    "            action = 2\n",
    "        if  action == 2 and self.lastMove == 3:\n",
    "            action = 3\n",
    "        \n",
    "        if action == 0:\n",
    "            if snakeY > 0:\n",
    "                if self.screenMap[snakeY - 1][snakeX]  == 0.5:\n",
    "                    gameOver = True\n",
    "                    reward = self.negReward\n",
    "                elif self.screenMap[snakeY - 1][snakeX] ==  1:\n",
    "                    reward = self.posReward\n",
    "                    self.moveSnake((snakeY - 1,snakeX),True)\n",
    "                elif self.screenMap[snakeY - 1][snakeX] == 0:\n",
    "                    self.moveSnake((snakeY - 1,snakeX),False)\n",
    "            else:\n",
    "                gameOver = True\n",
    "                reward = self.negReward\n",
    "        \n",
    "        elif action == 1:\n",
    "            if snakeY < self.nRows - 1:\n",
    "                if self.screenMap[snakeY + 1][snakeX] == 0.5:\n",
    "                    gameOver = True\n",
    "                    reward = self.negReward\n",
    "                elif self.screenMap[snakeY + 1][snakeX] == 1:\n",
    "                    reward = self.posReward\n",
    "                    self.moveSnake((snakeY + 1,snakeX),True)\n",
    "                elif self.screenMap[snakeY + 1][snakeX] == 0:\n",
    "                    self.moveSnake((snakeY + 1,snakeX),False)\n",
    "            else:\n",
    "                gameOver = True\n",
    "                reward = self.negReward\n",
    "\n",
    "        elif action == 2:\n",
    "            if snakeX < self.nColumns - 1:\n",
    "                if self.screenMap[snakeY][snakeX + 1] == 0.5:\n",
    "                    gameOver = True\n",
    "                    reward = self.negReward\n",
    "                elif self.screenMap[snakeY][snakeX + 1] == 1:\n",
    "                    reward = self.posReward\n",
    "                    self.moveSnake((snakeY,snakeX + 1),True)\n",
    "                elif self.screenMap[snakeY][snakeX + 1] == 0:\n",
    "                    self.moveSnake((snakeY,snakeX + 1),False)\n",
    "            else:\n",
    "                gameOver = True\n",
    "                reward = self.negReward\n",
    "\n",
    "        elif action == 3:\n",
    "            if snakeX > 0:\n",
    "                if self.screenMap[snakeY][snakeX - 1] == 0.5:\n",
    "                    gameOver = True\n",
    "                    reward = self.negReward\n",
    "                elif self.screenMap[snakeY][snakeX - 1] == 1:\n",
    "                    reward = self.posReward\n",
    "                    self.moveSnake((snakeY,snakeX - 1),True)\n",
    "                elif  self.screenMap[snakeY][snakeX - 1] == 0:\n",
    "                    self.moveSnake((snakeY,snakeX - 1),False)\n",
    "            else:\n",
    "                gameOver = True\n",
    "                reward = self.negReward\n",
    "        \n",
    "        self.drawScreen()\n",
    "        self.lastMove = action\n",
    "        pg.time.wait(self.waitTime)\n",
    "        return self.screenMap,reward,gameOver\n",
    "\n",
    "    def reset(self):\n",
    "        self.screenMap = np.zeros((self.nRows,self.nColumns))\n",
    "        self.snakePos = list()\n",
    "        \n",
    "        for i  in range(self.initSnakelen):\n",
    "            self.snakePos.append((int(self.nRows/2)+i,int(self.nColumns/2)))\n",
    "            self.screenMap[int(self.nRows/2)+i][int(self.nColumns/2)] = 0.5\n",
    "        self.screenMap[self.applePos[0]][self.applePos[1]] = 1\n",
    "        self.lastMove = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN\n",
    "class Brain():\n",
    "    def __init__(self,iS=(100,100,3),lr=0.0005):\n",
    "        self.learning_rate = lr\n",
    "        self.input_shape = iS\n",
    "        self.numOutput = 4\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=self.input_shape),\n",
    "            tf.keras.layers.MaxPool2D((2,2)),\n",
    "            tf.keras.layers.Conv2D(64,(2,2),activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(units=256,activation='relu'),\n",
    "            tf.keras.layers.Dense(units=self.numOutput)\n",
    "        ])\n",
    "        self.model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "    def loadModel(self,filepath):\n",
    "        self.model = tf.keras.models.load_model(filepath)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agente\n",
    "class DQN(object):\n",
    "    def __init__(self,max_memory=100,disount=0.9):\n",
    "        self.memory = list()\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = disount\n",
    "    \n",
    "    def remember(self,transition,game_over):\n",
    "        self.memory.append([transition,game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def get_batch(self,model,batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        inputs = np.zeros((min(len_memory,batch_size),self.memory[0][0][0].shape[1],self.memory[0][0][0].shape[2],self.memory[0][0][0].shape[3]))\n",
    "        num_ouput = model.output_shape[-1]\n",
    "        targets = np.zeros((min(len_memory,batch_size),num_ouput))\n",
    "        \n",
    "        for i,dx in enumerate(np.random.randint(0,len_memory,size=min(len_memory,batch_size))):\n",
    "            current_state,action,reward,next_state = self.memory[dx][0]\n",
    "            game_over = self.memory[dx][1]\n",
    "            inputs[i] = current_state\n",
    "            targets[i] = model.predict(current_state)[0]\n",
    "            Q_sam = np.max(model.predict(next_state)[0])\n",
    "            if game_over :\n",
    "                targets[i,action] = reward\n",
    "            else:\n",
    "                targets[i,action] = reward + self.discount*Q_sam\n",
    "            \n",
    "        return inputs,targets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Current  Best: 0 Epsilon: 0.99980\n",
      "Epoch: 2 Current  Best: 0 Epsilon: 0.99960\n",
      "Epoch: 3 Current  Best: 0 Epsilon: 0.99940\n",
      "Epoch: 4 Current  Best: 0 Epsilon: 0.99920\n",
      "Epoch: 5 Current  Best: 0 Epsilon: 0.99900\n",
      "Epoch: 6 Current  Best: 0 Epsilon: 0.99880\n",
      "Epoch: 7 Current  Best: 0 Epsilon: 0.99860\n",
      "Epoch: 8 Current  Best: 0 Epsilon: 0.99840\n",
      "Epoch: 9 Current  Best: 0 Epsilon: 0.99820\n",
      "Epoch: 10 Current  Best: 0 Epsilon: 0.99800\n",
      "Epoch: 11 Current  Best: 0 Epsilon: 0.99780\n",
      "Epoch: 12 Current  Best: 0 Epsilon: 0.99760\n",
      "Epoch: 13 Current  Best: 0 Epsilon: 0.99740\n",
      "Epoch: 14 Current  Best: 0 Epsilon: 0.99720\n",
      "Epoch: 15 Current  Best: 0 Epsilon: 0.99700\n",
      "Epoch: 16 Current  Best: 0 Epsilon: 0.99680\n",
      "Epoch: 17 Current  Best: 0 Epsilon: 0.99660\n",
      "Epoch: 18 Current  Best: 0 Epsilon: 0.99640\n",
      "Epoch: 19 Current  Best: 0 Epsilon: 0.99620\n",
      "Epoch: 20 Current  Best: 0 Epsilon: 0.99600\n",
      "Epoch: 21 Current  Best: 0 Epsilon: 0.99580\n",
      "Epoch: 22 Current  Best: 0 Epsilon: 0.99560\n",
      "Epoch: 23 Current  Best: 0 Epsilon: 0.99540\n",
      "Epoch: 24 Current  Best: 0 Epsilon: 0.99520\n",
      "Epoch: 25 Current  Best: 0 Epsilon: 0.99500\n",
      "Epoch: 26 Current  Best: 0 Epsilon: 0.99480\n",
      "Epoch: 27 Current  Best: 0 Epsilon: 0.99460\n",
      "Epoch: 28 Current  Best: 0 Epsilon: 0.99440\n",
      "Epoch: 29 Current  Best: 0 Epsilon: 0.99420\n",
      "Epoch: 30 Current  Best: 0 Epsilon: 0.99400\n",
      "Epoch: 31 Current  Best: 0 Epsilon: 0.99380\n",
      "Epoch: 32 Current  Best: 0 Epsilon: 0.99360\n",
      "Epoch: 33 Current  Best: 0 Epsilon: 0.99340\n",
      "Epoch: 34 Current  Best: 0 Epsilon: 0.99320\n",
      "Epoch: 35 Current  Best: 0 Epsilon: 0.99300\n",
      "Epoch: 36 Current  Best: 0 Epsilon: 0.99280\n",
      "Epoch: 37 Current  Best: 0 Epsilon: 0.99260\n",
      "Epoch: 38 Current  Best: 0 Epsilon: 0.99240\n",
      "Epoch: 39 Current  Best: 0 Epsilon: 0.99220\n",
      "Epoch: 40 Current  Best: 0 Epsilon: 0.99200\n",
      "Epoch: 41 Current  Best: 0 Epsilon: 0.99180\n",
      "Epoch: 42 Current  Best: 0 Epsilon: 0.99160\n",
      "Epoch: 43 Current  Best: 0 Epsilon: 0.99140\n",
      "Epoch: 44 Current  Best: 0 Epsilon: 0.99120\n",
      "Epoch: 45 Current  Best: 0 Epsilon: 0.99100\n",
      "Epoch: 46 Current  Best: 0 Epsilon: 0.99080\n",
      "Epoch: 47 Current  Best: 0 Epsilon: 0.99060\n",
      "Epoch: 48 Current  Best: 0 Epsilon: 0.99040\n",
      "Epoch: 49 Current  Best: 0 Epsilon: 0.99020\n",
      "Epoch: 50 Current  Best: 0 Epsilon: 0.99000\n",
      "Epoch: 51 Current  Best: 0 Epsilon: 0.98980\n",
      "Epoch: 52 Current  Best: 0 Epsilon: 0.98960\n",
      "Epoch: 53 Current  Best: 0 Epsilon: 0.98940\n",
      "Epoch: 54 Current  Best: 0 Epsilon: 0.98920\n",
      "Epoch: 55 Current  Best: 0 Epsilon: 0.98900\n",
      "Epoch: 56 Current  Best: 0 Epsilon: 0.98880\n",
      "Epoch: 57 Current  Best: 0 Epsilon: 0.98860\n",
      "Epoch: 58 Current  Best: 0 Epsilon: 0.98840\n",
      "Epoch: 59 Current  Best: 0 Epsilon: 0.98820\n",
      "Epoch: 60 Current  Best: 0 Epsilon: 0.98800\n",
      "Epoch: 61 Current  Best: 0 Epsilon: 0.98780\n",
      "Epoch: 62 Current  Best: 0 Epsilon: 0.98760\n",
      "Epoch: 63 Current  Best: 0 Epsilon: 0.98740\n",
      "Epoch: 64 Current  Best: 0 Epsilon: 0.98720\n",
      "Epoch: 65 Current  Best: 0 Epsilon: 0.98700\n",
      "Epoch: 66 Current  Best: 0 Epsilon: 0.98680\n",
      "Epoch: 67 Current  Best: 0 Epsilon: 0.98660\n",
      "Epoch: 68 Current  Best: 0 Epsilon: 0.98640\n",
      "Epoch: 69 Current  Best: 0 Epsilon: 0.98620\n",
      "Epoch: 70 Current  Best: 0 Epsilon: 0.98600\n",
      "Epoch: 71 Current  Best: 0 Epsilon: 0.98580\n",
      "Epoch: 72 Current  Best: 0 Epsilon: 0.98560\n",
      "Epoch: 73 Current  Best: 0 Epsilon: 0.98540\n",
      "Epoch: 74 Current  Best: 0 Epsilon: 0.98520\n",
      "Epoch: 75 Current  Best: 0 Epsilon: 0.98500\n",
      "Epoch: 76 Current  Best: 0 Epsilon: 0.98480\n",
      "Epoch: 77 Current  Best: 0 Epsilon: 0.98460\n",
      "Epoch: 78 Current  Best: 0 Epsilon: 0.98440\n",
      "Epoch: 79 Current  Best: 0 Epsilon: 0.98420\n",
      "Epoch: 80 Current  Best: 0 Epsilon: 0.98400\n",
      "Epoch: 81 Current  Best: 0 Epsilon: 0.98380\n",
      "Epoch: 82 Current  Best: 0 Epsilon: 0.98360\n",
      "Epoch: 83 Current  Best: 0 Epsilon: 0.98340\n",
      "Epoch: 84 Current  Best: 0 Epsilon: 0.98320\n",
      "Epoch: 85 Current  Best: 0 Epsilon: 0.98300\n",
      "Epoch: 86 Current  Best: 0 Epsilon: 0.98280\n",
      "Epoch: 87 Current  Best: 0 Epsilon: 0.98260\n",
      "Epoch: 88 Current  Best: 0 Epsilon: 0.98240\n",
      "Epoch: 89 Current  Best: 0 Epsilon: 0.98220\n",
      "Epoch: 90 Current  Best: 0 Epsilon: 0.98200\n",
      "Epoch: 91 Current  Best: 0 Epsilon: 0.98180\n",
      "Epoch: 92 Current  Best: 0 Epsilon: 0.98160\n",
      "Epoch: 93 Current  Best: 0 Epsilon: 0.98140\n",
      "Epoch: 94 Current  Best: 0 Epsilon: 0.98120\n",
      "Epoch: 95 Current  Best: 0 Epsilon: 0.98100\n",
      "Epoch: 96 Current  Best: 0 Epsilon: 0.98080\n",
      "Epoch: 97 Current  Best: 0 Epsilon: 0.98060\n",
      "Epoch: 98 Current  Best: 0 Epsilon: 0.98040\n",
      "Epoch: 99 Current  Best: 0 Epsilon: 0.98020\n",
      "Epoch: 100 Current  Best: 0 Epsilon: 0.98000\n",
      "Epoch: 101 Current  Best: 0 Epsilon: 0.97980\n",
      "Epoch: 102 Current  Best: 0 Epsilon: 0.97960\n",
      "Epoch: 103 Current  Best: 0 Epsilon: 0.97940\n",
      "Epoch: 104 Current  Best: 0 Epsilon: 0.97920\n",
      "Epoch: 105 Current  Best: 0 Epsilon: 0.97900\n",
      "Epoch: 106 Current  Best: 0 Epsilon: 0.97880\n",
      "Epoch: 107 Current  Best: 0 Epsilon: 0.97860\n",
      "Epoch: 108 Current  Best: 0 Epsilon: 0.97840\n",
      "Epoch: 109 Current  Best: 0 Epsilon: 0.97820\n",
      "Epoch: 110 Current  Best: 0 Epsilon: 0.97800\n",
      "Epoch: 111 Current  Best: 0 Epsilon: 0.97780\n",
      "Epoch: 112 Current  Best: 0 Epsilon: 0.97760\n",
      "Epoch: 113 Current  Best: 0 Epsilon: 0.97740\n",
      "Epoch: 114 Current  Best: 0 Epsilon: 0.97720\n",
      "Epoch: 115 Current  Best: 0 Epsilon: 0.97700\n",
      "Epoch: 116 Current  Best: 0 Epsilon: 0.97680\n",
      "Epoch: 117 Current  Best: 0 Epsilon: 0.97660\n",
      "Epoch: 118 Current  Best: 0 Epsilon: 0.97640\n",
      "Epoch: 119 Current  Best: 0 Epsilon: 0.97620\n",
      "Epoch: 120 Current  Best: 0 Epsilon: 0.97600\n",
      "Epoch: 121 Current  Best: 0 Epsilon: 0.97580\n",
      "Epoch: 122 Current  Best: 0 Epsilon: 0.97560\n",
      "Epoch: 123 Current  Best: 0 Epsilon: 0.97540\n",
      "Epoch: 124 Current  Best: 0 Epsilon: 0.97520\n",
      "Epoch: 125 Current  Best: 0 Epsilon: 0.97500\n",
      "Epoch: 126 Current  Best: 0 Epsilon: 0.97480\n",
      "Epoch: 127 Current  Best: 0 Epsilon: 0.97460\n",
      "Epoch: 128 Current  Best: 0 Epsilon: 0.97440\n",
      "Epoch: 129 Current  Best: 0 Epsilon: 0.97420\n",
      "Epoch: 130 Current  Best: 0 Epsilon: 0.97400\n",
      "Epoch: 131 Current  Best: 0 Epsilon: 0.97380\n",
      "Epoch: 132 Current  Best: 0 Epsilon: 0.97360\n",
      "Epoch: 133 Current  Best: 0 Epsilon: 0.97340\n",
      "Epoch: 134 Current  Best: 0 Epsilon: 0.97320\n",
      "Epoch: 135 Current  Best: 0 Epsilon: 0.97300\n",
      "Epoch: 136 Current  Best: 0 Epsilon: 0.97280\n",
      "Epoch: 137 Current  Best: 0 Epsilon: 0.97260\n",
      "Epoch: 138 Current  Best: 0 Epsilon: 0.97240\n",
      "Epoch: 139 Current  Best: 0 Epsilon: 0.97220\n",
      "Epoch: 140 Current  Best: 0 Epsilon: 0.97200\n",
      "Epoch: 141 Current  Best: 0 Epsilon: 0.97180\n",
      "Epoch: 142 Current  Best: 0 Epsilon: 0.97160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6e8ee64700a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mnextState\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnextState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnextState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgameOver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollected\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-5658f21be8a7>\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(self, model, batch_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mQ_sam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgame_over\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m     \"\"\"\n\u001b[1;32m-> 1615\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1617\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   3966\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3967\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3968\u001b[1;33m         **self._flat_structure)\n\u001b[0m\u001b[0;32m   3969\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   1728\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FlatMapDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1730\u001b[1;33m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   1731\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the AI\n",
    "menSize = 60000\n",
    "batchsize = 32\n",
    "learningrate = 0.0001\n",
    "gamma = 0.9\n",
    "nLastStates = 4\n",
    "epsilon = 1.\n",
    "epsilon_decay = 0.0002\n",
    "min_epsilon = 0.05\n",
    "\n",
    "env = Enviroment(0)\n",
    "brain = Brain((env.nRows,env.nColumns,nLastStates),learningrate)\n",
    "model = brain.model\n",
    "dqn = DQN(menSize,gamma)\n",
    "\n",
    "def resetState():\n",
    "    currentState = np.zeros((1,env.nRows,env.nColumns,nLastStates))\n",
    "\n",
    "    for i in range(nLastStates):\n",
    "        currentState[:,:,:,i] = env.screenMap\n",
    "    \n",
    "    return  currentState,currentState\n",
    "\n",
    "epoch = 0\n",
    "scores = list()\n",
    "maxNCollected = 0\n",
    "nCollected = 0.\n",
    "totNCollected =  0\n",
    "\n",
    "while True:\n",
    "    env.reset()\n",
    "    currentState,nextState = resetState()\n",
    "    epoch += 1\n",
    "    gameOver = False\n",
    "\n",
    "    while not gameOver:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(0,4)\n",
    "        else:\n",
    "            qvalue = model.predict(currentState)[0]\n",
    "            action = np.argmax(qvalue)\n",
    "        state,reward,gameOver = env.step(action)\n",
    "        state = np.reshape(state,(1,env.nRows,env.nColumns,1))\n",
    "        nextState = np.append(nextState,state,axis=3)\n",
    "        nextState = np.delete(nextState,0,axis=3)\n",
    "        dqn.remember([currentState,action,reward,nextState],gameOver)\n",
    "        inputs,targats = dqn.get_batch(model,batchsize)\n",
    "        model.train_on_batch(inputs,targats)\n",
    "        if env.collected:\n",
    "            nCollected += 1\n",
    "        currentState = nextState\n",
    "    if nCollected > maxNCollected and nCollected > 2:\n",
    "        maxNCollected = nCollected\n",
    "    totNCollected += nCollected\n",
    "    nCollected = 0\n",
    "\n",
    "    if epoch % 100 == 0 and epoch != 0 :\n",
    "        scores.append(totNCollected/100)\n",
    "        totNCollected = 0\n",
    "        plt.plot(scores)\n",
    "        plt.xlabel('Epoch / 100')\n",
    "        plt.ylabel('Average Score')\n",
    "        plt.close()\n",
    "    \n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon -= epsilon_decay\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+ ' Current  Best: '+str(maxNCollected)+' Epsilon: {:.5f}'.format(epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
